version: '2'
services:

# NOTE: static IP addresses are currently only supported in docker-compose v2

# The CLI container is used to execute Conjur commands in lieu of requiring the CLI
# package installation on the host machine. This makes managing multi-version Conjur
# environments easier.

  conjur1:
    image: conjur-appliance:latest
    container_name: conjur1     # important that container name == service name
    labels:
      role: "conjur_node"
    volumes:
      - ./:/src:z
      - ./log:/var/log/conjur   # exported conjur audit log
      - ./log:/var/log/nginx    # exported nginx audit log
    security_opt:
      - seccomp:unconfined
    restart: always
    networks:
      ntwk:
        ipv4_address: "10.5.0.2"

  conjur2:
    image: conjur-appliance:latest
    container_name: conjur2     # important that container name == service name
    labels:
      role: "conjur_node"
    volumes:
      - ./:/src:z
      - ./log:/var/log/conjur   # exported conjur audit log
      - ./log:/var/log/nginx    # exported nginx audit log
    security_opt:
      - seccomp:unconfined
    restart: always
    networks:
      ntwk:
        ipv4_address: "10.5.0.13"      # upper range of addresses, hopefully unused when created

  conjur3:
    image: conjur-appliance:latest
    container_name: conjur3     # important that container name == service name
    labels:
      role: "conjur_node"
    volumes:
      - ./:/src:z
      - ./log:/var/log/conjur   # exported conjur audit log
      - ./log:/var/log/nginx    # exported nginx audit log
    security_opt:
      - seccomp:unconfined
    restart: always
    networks:
      ntwk:
        ipv4_address: "10.5.0.14"      # upper range of addresses, hopefully unused when created

  haproxy:
    image: haproxy:conjur
    hostname: conjur_proxy
    container_name: conjur_master
    build: ./build/haproxy
    labels:
      role: "conjur_proxy"
    volumes:
      - ./:/src:z
    ports:
      - 443:443
    restart: always
    entrypoint: "/start.sh"
    networks:
      - ntwk

  etcd: 
    image: quay.io/coreos/etcd
    hostname: etcd
    build: ./build/etcd
    command:
      - etcd
      - -debug
      - -name
      - etcd
      - -advertise-client-urls
      - http://etcd:2379
      - -listen-client-urls
      - http://0.0.0.0:2379
      - -initial-advertise-peer-urls
      - http://etcd:2380
      - -listen-peer-urls
      - http://0.0.0.0:2380
      - -initial-cluster
      - etcd=http://etcd:2380
    networks:
      - ntwk

  hsm:
    image: softhsm:latest
    build: ./build/hsm
    networks:
      - ntwk

  follower:
    image: conjur-appliance:latest
    labels:
      role: "conjur_follower"
    volumes:
      - ./:/src:z
    security_opt:
      - seccomp:unconfined
    restart: always
    networks:
      - ntwk

  cli:
    environment:
      CONJUR_ACCOUNT: dev
      CONJUR_APPLIANCE_URL: https://conjur_master/api
    hostname: conjurcli
    image: my-conjurcli:5.4.0
    build: ./build/conjurcli
    volumes:
      - data:/data
      - ./:/src:z
      - "/var/run/docker.sock:/var/run/docker.sock:rw"  # enable docker commands from in container
      - "/usr/bin/docker:/usr/bin/docker:z"
    entrypoint: sleep
    command: infinity
    networks:
      - ntwk

  scope:
    image: weaveworks/scope:1.6.5
    privileged: true
    ports:
      - "0.0.0.0:4040:4040"
    labels:
      - "works.weave.role=system"
    volumes:
      - "/var/run/docker.sock:/var/run/docker.sock:rw"
    command:
      - "--probe.docker=true"
    networks:
      - ntwk

# The webapp service is just a simple script running in a container - not really a web app.
# This service is brought up by the 1-setup-containers.sh script.
  webapp:
    image: webapp
    build: ./build/webapp
    volumes:
      - data:/data
    entrypoint: /root/webapp1.sh
    environment:
      - APP_HOSTNAME         # values for these variables are in .env file
      - VAR_ID               # written by 1-setup-containers.sh
      - SLEEP_TIME
    networks:
      - ntwk

# VM containers for SSH management demonstration.
# This service is brought up by the ./ssh/0-setup-ssh.sh script.
  vm:
    image: rack-vm:1.0
    build: ./build/vm
    volumes:
      - .:/src
      - data:/data
    entrypoint: sleep
    command: infinity
    networks:
      - ntwk

# Open LDAP server for ldap sync demonstration.
# This service is brought up by the ./ldap/0-setup-ldap.sh script.
  ldap:
    container_name: ldap_server
    image: osixia/openldap:1.1.7
    build: ./build/ldap
    restart: always
    volumes:
      - .:/src
    networks:
      - ntwk

# Splunk enterprise server for Splunk monitoring demonstration.
# This requires the Conjur and Nginx logs be exported from the Conjur container.
# See volumes: specification for the Conjur service above.
# This service is brought up by the ./splunk/0-setup-splunk.sh script.
  splunk:
    container_name: splunk
    hostname: splunkenterprise
    image: splunk/splunk:7.0.0
    build: ./build/splunk
    environment:
      SPLUNK_START_ARGS: --accept-license
      SPLUNK_ENABLE_LISTEN: 9997
      SPLUNK_ADD: tcp 1514
    volumes:
      - ./log:/log
      - opt-splunk-etc:/opt/splunk/etc
      - opt-splunk-var:/opt/splunk/var
    ports:
      - "8000:8000"
      - "9997:9997"
      - "8088:8088"
      - "1514:1514"
    networks:
      - ntwk

  ansible:
    container_name: ansible
    image: "ansible:centos7"
    build: build/ansible
    environment:
      ANSIBLE_HOST_KEY_CHECKING: "false"
    volumes:
      - ./:/src:rw
      - ./ssh_ansible/ansible_hosts:/etc/ansible/hosts:rw
    entrypoint: "sleep infinity"
    networks:
      - ntwk

volumes:
  data:
  opt-splunk-etc:
  opt-splunk-var:

networks:
  ntwk:
    driver: bridge
    ipam:
     config:
       - subnet: 10.5.0.0/16
         gateway: 10.5.0.1
